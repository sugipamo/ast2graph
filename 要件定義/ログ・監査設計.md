# ast2graph ログ・監査設計書

## 1. 概要

ast2graphライブラリにおけるログ出力と監査証跡の設計方針を定義します。

## 2. ログ設計の基本方針

### 2.1 ログレベル定義

| レベル | 用途 | 例 |
|--------|------|---|
| **CRITICAL** | システム停止レベルの致命的エラー | Neo4j接続完全失敗、メモリ枯渇 |
| **ERROR** | 処理失敗（リカバリ不可） | ファイル読み込み失敗、構文エラー |
| **WARNING** | 処理継続可能な問題 | 非推奨機能使用、パフォーマンス低下 |
| **INFO** | 重要な処理の進行状況 | 処理開始/終了、マイルストーン |
| **DEBUG** | 詳細なデバッグ情報 | 内部状態、変数値、処理フロー |

### 2.2 構造化ログ

```python
import logging
import json
from datetime import datetime
from typing import Any, Dict, Optional
import traceback

class StructuredLogger:
    """構造化ログ出力クラス"""
    
    def __init__(self, name: str):
        self.logger = logging.getLogger(name)
        self.context = {}
    
    def set_context(self, **kwargs):
        """永続的なコンテキスト情報を設定"""
        self.context.update(kwargs)
    
    def _format_log_entry(
        self,
        level: str,
        message: str,
        **kwargs
    ) -> Dict[str, Any]:
        """ログエントリを構造化"""
        entry = {
            "timestamp": datetime.utcnow().isoformat(),
            "level": level,
            "logger": self.logger.name,
            "message": message,
            "context": {**self.context, **kwargs}
        }
        
        # エラーの場合はスタックトレースを追加
        if level in ["ERROR", "CRITICAL"]:
            entry["traceback"] = traceback.format_exc()
        
        return entry
    
    def info(self, message: str, **kwargs):
        log_entry = self._format_log_entry("INFO", message, **kwargs)
        self.logger.info(json.dumps(log_entry))
    
    def error(self, message: str, error: Optional[Exception] = None, **kwargs):
        if error:
            kwargs["error_type"] = error.__class__.__name__
            kwargs["error_details"] = str(error)
        
        log_entry = self._format_log_entry("ERROR", message, **kwargs)
        self.logger.error(json.dumps(log_entry))
```

## 3. ログカテゴリ設計

### 3.1 カテゴリ別ロガー

```python
class LoggerFactory:
    """用途別ロガーの生成"""
    
    @staticmethod
    def get_logger(category: str) -> StructuredLogger:
        logger_map = {
            "parser": "ast2graph.parser",
            "graph": "ast2graph.graph",
            "neo4j": "ast2graph.neo4j",
            "performance": "ast2graph.performance",
            "security": "ast2graph.security",
            "audit": "ast2graph.audit"
        }
        
        logger_name = logger_map.get(category, f"ast2graph.{category}")
        return StructuredLogger(logger_name)

# 使用例
parser_logger = LoggerFactory.get_logger("parser")
neo4j_logger = LoggerFactory.get_logger("neo4j")
```

### 3.2 パフォーマンスログ

```python
import time
from contextlib import contextmanager

class PerformanceLogger:
    """パフォーマンス計測用ログ"""
    
    def __init__(self):
        self.logger = LoggerFactory.get_logger("performance")
    
    @contextmanager
    def measure(self, operation: str, **context):
        """処理時間を計測してログ出力"""
        start_time = time.time()
        
        self.logger.debug(f"{operation} started", 
                         operation=operation,
                         **context)
        
        try:
            yield
            
            duration = time.time() - start_time
            self.logger.info(f"{operation} completed",
                           operation=operation,
                           duration_seconds=duration,
                           status="success",
                           **context)
            
        except Exception as e:
            duration = time.time() - start_time
            self.logger.error(f"{operation} failed",
                            operation=operation,
                            duration_seconds=duration,
                            status="failed",
                            error=e,
                            **context)
            raise

# 使用例
perf_logger = PerformanceLogger()

with perf_logger.measure("parse_file", file_path="/path/to/file.py"):
    # AST解析処理
    ast_tree = parse_ast(source_code)
```

## 4. ログ出力設定

### 4.1 環境別ログ設定

```python
import logging.config
import os

def configure_logging(environment: str = None):
    """環境に応じたログ設定"""
    
    env = environment or os.getenv("AST2GRAPH_ENV", "development")
    
    if env == "production":
        config = PRODUCTION_LOG_CONFIG
    elif env == "development":
        config = DEVELOPMENT_LOG_CONFIG
    else:
        config = DEFAULT_LOG_CONFIG
    
    logging.config.dictConfig(config)

PRODUCTION_LOG_CONFIG = {
    "version": 1,
    "disable_existing_loggers": False,
    "formatters": {
        "json": {
            "class": "pythonjsonlogger.jsonlogger.JsonFormatter",
            "format": "%(timestamp)s %(level)s %(name)s %(message)s"
        }
    },
    "handlers": {
        "file": {
            "class": "logging.handlers.RotatingFileHandler",
            "formatter": "json",
            "filename": "/var/log/ast2graph/app.log",
            "maxBytes": 104857600,  # 100MB
            "backupCount": 10,
            "level": "INFO"
        },
        "error_file": {
            "class": "logging.handlers.RotatingFileHandler",
            "formatter": "json",
            "filename": "/var/log/ast2graph/error.log",
            "maxBytes": 52428800,   # 50MB
            "backupCount": 5,
            "level": "ERROR"
        }
    },
    "loggers": {
        "ast2graph": {
            "handlers": ["file", "error_file"],
            "level": "INFO",
            "propagate": False
        }
    }
}

DEVELOPMENT_LOG_CONFIG = {
    "version": 1,
    "disable_existing_loggers": False,
    "formatters": {
        "console": {
            "format": "%(asctime)s [%(levelname)s] %(name)s: %(message)s"
        }
    },
    "handlers": {
        "console": {
            "class": "logging.StreamHandler",
            "formatter": "console",
            "level": "DEBUG"
        }
    },
    "loggers": {
        "ast2graph": {
            "handlers": ["console"],
            "level": "DEBUG",
            "propagate": False
        }
    }
}
```

### 4.2 ログローテーション

```python
from logging.handlers import TimedRotatingFileHandler

class LogRotationConfig:
    """ログローテーション設定"""
    
    @staticmethod
    def create_rotating_handler(
        filename: str,
        when: str = "midnight",
        interval: int = 1,
        backup_count: int = 30
    ):
        """時間ベースのローテーションハンドラを作成"""
        
        handler = TimedRotatingFileHandler(
            filename=filename,
            when=when,
            interval=interval,
            backupCount=backup_count,
            encoding="utf-8"
        )
        
        # ファイル名のフォーマット
        handler.suffix = "%Y%m%d"
        
        return handler
```

## 5. 監査ログ設計

### 5.1 監査イベント定義

```python
from enum import Enum
from dataclasses import dataclass
from typing import Optional

class AuditEventType(Enum):
    # データ操作
    DATA_CREATE = "data.create"
    DATA_UPDATE = "data.update"
    DATA_DELETE = "data.delete"
    DATA_QUERY = "data.query"
    
    # システムイベント
    SYSTEM_START = "system.start"
    SYSTEM_STOP = "system.stop"
    CONFIG_CHANGE = "config.change"
    
    # セキュリティイベント
    AUTH_SUCCESS = "auth.success"
    AUTH_FAILURE = "auth.failure"
    ACCESS_DENIED = "access.denied"

@dataclass
class AuditEvent:
    """監査イベント"""
    event_id: str
    event_type: AuditEventType
    timestamp: datetime
    user: Optional[str]
    source_ip: Optional[str]
    resource: Optional[str]
    action: str
    result: str
    details: Dict[str, Any]
```

### 5.2 監査ログ実装

```python
class AuditLogger:
    """監査ログ出力クラス"""
    
    def __init__(self, neo4j_driver=None):
        self.logger = LoggerFactory.get_logger("audit")
        self.driver = neo4j_driver
    
    def log_event(self, event: AuditEvent):
        """監査イベントをログ出力"""
        
        # ファイルログ
        self.logger.info(
            f"Audit event: {event.event_type.value}",
            event_id=event.event_id,
            event_type=event.event_type.value,
            user=event.user,
            resource=event.resource,
            action=event.action,
            result=event.result,
            details=event.details
        )
        
        # Neo4jに保存（オプション）
        if self.driver:
            self._store_in_neo4j(event)
    
    def _store_in_neo4j(self, event: AuditEvent):
        """監査ログをNeo4jに保存"""
        with self.driver.session() as session:
            session.run("""
                CREATE (a:AuditLog {
                    event_id: $event_id,
                    event_type: $event_type,
                    timestamp: $timestamp,
                    user: $user,
                    source_ip: $source_ip,
                    resource: $resource,
                    action: $action,
                    result: $result,
                    details: $details
                })
            """, 
                event_id=event.event_id,
                event_type=event.event_type.value,
                timestamp=event.timestamp.isoformat(),
                user=event.user,
                source_ip=event.source_ip,
                resource=event.resource,
                action=event.action,
                result=event.result,
                details=json.dumps(event.details)
            )
```

### 5.3 監査対象の操作

```python
from functools import wraps
import uuid

def audit_operation(event_type: AuditEventType, resource_type: str = "ast_graph"):
    """監査ログを出力するデコレータ"""
    
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            # 監査情報の収集
            event = AuditEvent(
                event_id=str(uuid.uuid4()),
                event_type=event_type,
                timestamp=datetime.utcnow(),
                user=get_current_user(),  # 実装に応じて
                source_ip=get_source_ip(),  # 実装に応じて
                resource=resource_type,
                action=func.__name__,
                result="pending",
                details={"args": str(args), "kwargs": str(kwargs)}
            )
            
            audit_logger = AuditLogger()
            
            try:
                # 関数実行
                result = func(*args, **kwargs)
                
                # 成功を記録
                event.result = "success"
                audit_logger.log_event(event)
                
                return result
                
            except Exception as e:
                # 失敗を記録
                event.result = "failure"
                event.details["error"] = str(e)
                audit_logger.log_event(event)
                raise
        
        return wrapper
    return decorator

# 使用例
@audit_operation(AuditEventType.DATA_CREATE)
def create_graph(source_id: str, graph_data: GraphStructure):
    # グラフ作成処理
    pass
```

## 6. 機密情報の保護

### 6.1 ログフィルタリング

```python
import re

class SensitiveDataFilter(logging.Filter):
    """機密情報をログから除外するフィルター"""
    
    # 除外パターン
    PATTERNS = [
        (r'password["\']?\s*[:=]\s*["\']?([^"\'}\s]+)', 'password=[REDACTED]'),
        (r'NEO4J_PASSWORD=\S+', 'NEO4J_PASSWORD=[REDACTED]'),
        (r'auth=\([^)]+\)', 'auth=([REDACTED])'),
        (r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b', '[EMAIL_REDACTED]'),
        (r'\b(?:\d{4}[-\s]?){3}\d{4}\b', '[CREDIT_CARD_REDACTED]')
    ]
    
    def filter(self, record: logging.LogRecord) -> bool:
        """ログメッセージから機密情報を除去"""
        
        if hasattr(record, 'msg'):
            message = str(record.msg)
            
            for pattern, replacement in self.PATTERNS:
                message = re.sub(pattern, replacement, message)
            
            record.msg = message
        
        return True

# フィルターの適用
logger = logging.getLogger("ast2graph")
logger.addFilter(SensitiveDataFilter())
```

### 6.2 ソースコード内容の保護

```python
class SourceCodeLogger:
    """ソースコードを含むログの制御"""
    
    def __init__(self, include_source: bool = False):
        self.include_source = include_source
        self.logger = LoggerFactory.get_logger("parser")
    
    def log_parse_error(
        self,
        file_path: str,
        error: Exception,
        source_snippet: Optional[str] = None
    ):
        """構文エラーをログ（ソースコードの扱いに注意）"""
        
        log_data = {
            "file_path": file_path,
            "error_type": error.__class__.__name__,
            "error_message": str(error)
        }
        
        if self.include_source and source_snippet:
            # 開発環境のみソースコードを含める
            log_data["source_snippet"] = source_snippet[:200]  # 最大200文字
        else:
            # 本番環境ではハッシュのみ
            log_data["source_hash"] = hashlib.sha256(
                source_snippet.encode() if source_snippet else b""
            ).hexdigest()[:8]
        
        self.logger.error("Parse error occurred", **log_data)
```

## 7. ログ分析とモニタリング

### 7.1 メトリクス収集

```python
from collections import defaultdict
from typing import Counter

class LogMetricsCollector:
    """ログからメトリクスを収集"""
    
    def __init__(self):
        self.metrics = defaultdict(Counter)
    
    def collect_from_log_file(self, log_file: str):
        """ログファイルからメトリクスを収集"""
        
        with open(log_file) as f:
            for line in f:
                try:
                    log_entry = json.loads(line)
                    self._process_log_entry(log_entry)
                except json.JSONDecodeError:
                    continue
    
    def _process_log_entry(self, entry: dict):
        """ログエントリを処理"""
        
        # レベル別カウント
        level = entry.get("level", "UNKNOWN")
        self.metrics["log_levels"][level] += 1
        
        # エラータイプ別カウント
        if level == "ERROR":
            error_type = entry.get("context", {}).get("error_type", "Unknown")
            self.metrics["error_types"][error_type] += 1
        
        # 処理時間の収集
        if "duration_seconds" in entry.get("context", {}):
            operation = entry.get("context", {}).get("operation", "unknown")
            duration = entry["context"]["duration_seconds"]
            self.metrics["operation_durations"][operation].append(duration)
    
    def get_summary(self) -> dict:
        """収集したメトリクスのサマリーを取得"""
        
        summary = {
            "log_levels": dict(self.metrics["log_levels"]),
            "error_types": dict(self.metrics["error_types"]),
            "operation_stats": {}
        }
        
        # 処理時間の統計
        for operation, durations in self.metrics["operation_durations"].items():
            if durations:
                summary["operation_stats"][operation] = {
                    "count": len(durations),
                    "avg_duration": sum(durations) / len(durations),
                    "max_duration": max(durations),
                    "min_duration": min(durations)
                }
        
        return summary
```

### 7.2 アラート設定

```python
class LogAlertManager:
    """ログベースのアラート管理"""
    
    def __init__(self):
        self.alert_rules = []
        self.alert_handlers = []
    
    def add_rule(self, rule: Dict[str, Any]):
        """アラートルールを追加"""
        self.alert_rules.append(rule)
    
    def check_log_entry(self, log_entry: dict):
        """ログエントリをチェックしてアラート発火"""
        
        for rule in self.alert_rules:
            if self._matches_rule(log_entry, rule):
                self._trigger_alert(rule, log_entry)
    
    def _matches_rule(self, entry: dict, rule: dict) -> bool:
        """ログエントリがルールに一致するか確認"""
        
        # レベルチェック
        if "level" in rule and entry.get("level") != rule["level"]:
            return False
        
        # パターンマッチング
        if "pattern" in rule:
            message = entry.get("message", "")
            if not re.search(rule["pattern"], message):
                return False
        
        # 閾値チェック
        if "threshold" in rule:
            value = entry.get("context", {}).get(rule["metric"])
            if value is None or value < rule["threshold"]:
                return False
        
        return True

# アラートルールの例
alert_manager = LogAlertManager()

# エラー率のアラート
alert_manager.add_rule({
    "name": "high_error_rate",
    "level": "ERROR",
    "threshold": 10,  # 10エラー/分
    "action": "email"
})

# パフォーマンス低下のアラート
alert_manager.add_rule({
    "name": "slow_processing",
    "pattern": r"duration_seconds.*[1-9]\d{2,}",  # 100秒以上
    "action": "slack"
})
```

## 8. ログの保持とアーカイブ

### 8.1 保持ポリシー

```python
class LogRetentionPolicy:
    """ログ保持ポリシー"""
    
    DEFAULT_RETENTION_DAYS = {
        "debug": 7,
        "info": 30,
        "warning": 90,
        "error": 365,
        "critical": 730,  # 2年
        "audit": 2555     # 7年（コンプライアンス要件）
    }
    
    def cleanup_old_logs(self, log_directory: str):
        """古いログファイルをクリーンアップ"""
        
        for log_file in Path(log_directory).glob("*.log*"):
            # ファイル名からログレベルを推定
            level = self._get_log_level_from_filename(log_file.name)
            retention_days = self.DEFAULT_RETENTION_DAYS.get(level, 30)
            
            # ファイルの更新日時をチェック
            file_age_days = (datetime.now() - datetime.fromtimestamp(
                log_file.stat().st_mtime
            )).days
            
            if file_age_days > retention_days:
                # アーカイブまたは削除
                if level in ["error", "critical", "audit"]:
                    self._archive_log(log_file)
                else:
                    log_file.unlink()
```

## 9. 推奨設定

### 9.1 開発環境
```python
# 開発環境の推奨設定
DEV_LOG_CONFIG = {
    "log_level": "DEBUG",
    "output": "console",
    "format": "human_readable",
    "include_source": True,
    "sensitive_filter": False
}
```

### 9.2 本番環境
```python
# 本番環境の推奨設定
PROD_LOG_CONFIG = {
    "log_level": "INFO",
    "output": ["file", "syslog"],
    "format": "json",
    "include_source": False,
    "sensitive_filter": True,
    "rotation": "daily",
    "retention_days": 90,
    "audit_to_database": True
}
```

この設計により、ast2graphライブラリは包括的なログ記録と監査証跡を実現します。