# ast2graph 並列処理設計書

## 1. 概要

ast2graphライブラリにおける並列処理の設計方針と実装戦略を定義します。300ファイルの大規模処理に対応するための効率的な並列化を実現します。

## 2. 並列処理の基本方針

### 2.1 並列化の対象

| 処理レベル | 並列化可否 | 理由 |
|-----------|-----------|------|
| **ファイル単位** | ✅ 推奨 | 各ファイルは独立して処理可能 |
| **AST解析** | ✅ 可能 | CPU依存の処理 |
| **グラフ構築** | ✅ 可能 | メモリ内での独立処理 |
| **Neo4j書き込み** | ⚠️ 制限付き | トランザクション競合に注意 |

### 2.2 並列化戦略

```python
from enum import Enum

class ParallelStrategy(Enum):
    THREAD_POOL = "thread"      # I/Oバウンドタスク向け
    PROCESS_POOL = "process"    # CPUバウンドタスク向け
    ASYNC_IO = "asyncio"        # 非同期I/O
    HYBRID = "hybrid"           # 処理内容に応じて切り替え
```

## 3. ワーカー数の決定

### 3.1 自動最適化

```python
import os
import psutil
from typing import Optional

class WorkerOptimizer:
    """ワーカー数の最適化"""
    
    @staticmethod
    def get_optimal_workers(
        task_type: str,
        file_count: int,
        override: Optional[int] = None
    ) -> int:
        """最適なワーカー数を計算"""
        
        # 手動設定があれば優先
        if override:
            return override
        
        # CPU情報の取得
        cpu_count = os.cpu_count() or 4
        memory_gb = psutil.virtual_memory().total / (1024**3)
        
        if task_type == "ast_parsing":
            # CPU依存: コア数の80%を使用
            workers = int(cpu_count * 0.8)
            
        elif task_type == "neo4j_writing":
            # I/O依存: 接続プールサイズに制限
            # メモリとNeo4j接続数を考慮
            max_connections = 50  # Neo4j推奨値
            memory_per_worker_gb = 0.5  # 想定メモリ使用量
            
            workers = min(
                max_connections,
                int(memory_gb / memory_per_worker_gb),
                cpu_count * 2  # I/Oバウンドなので多めに
            )
            
        else:  # file_processing
            # ハイブリッド: ファイル数とリソースのバランス
            workers = min(
                cpu_count,
                file_count // 10,  # 1ワーカーあたり最低10ファイル
                int(memory_gb * 2)  # メモリ1GBあたり2ワーカー
            )
        
        return max(1, workers)  # 最低1ワーカー

# 使用例
optimizer = WorkerOptimizer()
workers = optimizer.get_optimal_workers("ast_parsing", file_count=300)
print(f"Optimal workers: {workers}")
```

### 3.2 動的調整

```python
class DynamicWorkerManager:
    """動的なワーカー数調整"""
    
    def __init__(self, initial_workers: int = 4):
        self.current_workers = initial_workers
        self.performance_history = []
        self.adjustment_interval = 60  # 60秒ごとに調整
    
    def adjust_workers(self, current_metrics: dict) -> int:
        """パフォーマンスメトリクスに基づいてワーカー数を調整"""
        
        cpu_usage = current_metrics.get("cpu_usage", 0)
        memory_usage = current_metrics.get("memory_usage", 0)
        queue_size = current_metrics.get("queue_size", 0)
        avg_processing_time = current_metrics.get("avg_processing_time", 0)
        
        # 調整ロジック
        if cpu_usage > 90 and memory_usage < 70:
            # CPU飽和、メモリ余裕 → ワーカー減
            self.current_workers = max(1, self.current_workers - 1)
            
        elif cpu_usage < 50 and queue_size > 100:
            # CPU余裕、キュー溜まり → ワーカー増
            self.current_workers = min(
                self.current_workers + 2,
                os.cpu_count() * 2
            )
            
        elif memory_usage > 85:
            # メモリ逼迫 → ワーカー減
            self.current_workers = max(1, self.current_workers - 2)
        
        return self.current_workers
```

## 4. 並列処理実装

### 4.1 マルチプロセス実装

```python
from concurrent.futures import ProcessPoolExecutor, as_completed
from typing import List, Callable, Any
import multiprocessing as mp

class ParallelProcessor:
    """並列処理の基本実装"""
    
    def __init__(self, max_workers: Optional[int] = None):
        self.max_workers = max_workers or WorkerOptimizer.get_optimal_workers(
            "file_processing", 300
        )
        self.results = []
        self.errors = []
    
    def process_files_parallel(
        self,
        file_paths: List[str],
        process_func: Callable[[str], Any],
        progress_callback: Optional[Callable] = None
    ) -> ProcessingResult:
        """ファイルを並列処理"""
        
        # プロセスプールの作成
        with ProcessPoolExecutor(max_workers=self.max_workers) as executor:
            # ジョブの投入
            future_to_file = {
                executor.submit(process_func, file_path): file_path
                for file_path in file_paths
            }
            
            # 結果の収集
            completed = 0
            for future in as_completed(future_to_file):
                file_path = future_to_file[future]
                completed += 1
                
                try:
                    result = future.result()
                    self.results.append((file_path, result))
                    
                except Exception as e:
                    self.errors.append((file_path, e))
                    logger.error(f"Failed to process {file_path}: {e}")
                
                # 進捗通知
                if progress_callback:
                    progress_callback(completed, len(file_paths))
        
        return ProcessingResult(
            successful=len(self.results),
            failed=len(self.errors),
            results=self.results,
            errors=self.errors
        )
```

### 4.2 非同期I/O実装

```python
import asyncio
import aiofiles
from neo4j import AsyncGraphDatabase

class AsyncProcessor:
    """非同期I/Oによる並列処理"""
    
    def __init__(self, neo4j_uri: str, auth: tuple):
        self.driver = AsyncGraphDatabase.driver(neo4j_uri, auth=auth)
        self.semaphore = asyncio.Semaphore(10)  # 同時実行数制限
    
    async def process_file_async(self, file_path: str) -> ProcessResult:
        """ファイルを非同期処理"""
        
        async with self.semaphore:  # 同時実行数を制限
            try:
                # ファイル読み込み
                async with aiofiles.open(file_path, 'r') as f:
                    content = await f.read()
                
                # AST解析（CPU処理なので別スレッドで）
                loop = asyncio.get_event_loop()
                ast_tree = await loop.run_in_executor(
                    None, parse_ast, content
                )
                
                # グラフ構築
                graph = await loop.run_in_executor(
                    None, build_graph, ast_tree, file_path
                )
                
                # Neo4jへの非同期書き込み
                await self.write_to_neo4j_async(graph)
                
                return ProcessResult(file_path, "success")
                
            except Exception as e:
                return ProcessResult(file_path, "failed", error=e)
    
    async def write_to_neo4j_async(self, graph: GraphStructure):
        """Neo4jへの非同期書き込み"""
        
        async with self.driver.session() as session:
            # バッチ処理
            batch = []
            for node in graph.nodes:
                batch.append(node)
                
                if len(batch) >= 1000:
                    await self._write_batch_async(session, batch)
                    batch = []
            
            # 残りを書き込み
            if batch:
                await self._write_batch_async(session, batch)
    
    async def process_all_files(self, file_paths: List[str]) -> List[ProcessResult]:
        """全ファイルを非同期並列処理"""
        
        tasks = [
            self.process_file_async(file_path)
            for file_path in file_paths
        ]
        
        # プログレスバー付き実行
        results = []
        for coro in asyncio.as_completed(tasks):
            result = await coro
            results.append(result)
            print(f"Progress: {len(results)}/{len(tasks)}")
        
        return results
```

### 4.3 ハイブリッド実装

```python
class HybridParallelProcessor:
    """CPU処理とI/O処理を最適に組み合わせた実装"""
    
    def __init__(self, cpu_workers: int = None, io_workers: int = None):
        self.cpu_workers = cpu_workers or os.cpu_count()
        self.io_workers = io_workers or os.cpu_count() * 2
        
        # CPU処理用プロセスプール
        self.cpu_pool = ProcessPoolExecutor(max_workers=self.cpu_workers)
        
        # I/O処理用スレッドプール
        self.io_pool = ThreadPoolExecutor(max_workers=self.io_workers)
        
        # パイプライン用キュー
        self.parse_queue = mp.Queue(maxsize=100)
        self.graph_queue = mp.Queue(maxsize=100)
    
    def process_pipeline(self, file_paths: List[str]):
        """パイプライン処理"""
        
        # Stage 1: ファイル読み込み（I/O）
        read_futures = []
        for file_path in file_paths:
            future = self.io_pool.submit(self._read_file, file_path)
            read_futures.append(future)
        
        # Stage 2: AST解析（CPU）
        parse_futures = []
        for future in as_completed(read_futures):
            content, file_path = future.result()
            parse_future = self.cpu_pool.submit(
                self._parse_ast, content, file_path
            )
            parse_futures.append(parse_future)
        
        # Stage 3: グラフ構築（CPU）
        graph_futures = []
        for future in as_completed(parse_futures):
            ast_tree, file_path = future.result()
            graph_future = self.cpu_pool.submit(
                self._build_graph, ast_tree, file_path
            )
            graph_futures.append(graph_future)
        
        # Stage 4: Neo4j書き込み（I/O）
        write_futures = []
        batch = []
        for future in as_completed(graph_futures):
            graph = future.result()
            batch.append(graph)
            
            if len(batch) >= 50:
                write_future = self.io_pool.submit(
                    self._write_batch, batch.copy()
                )
                write_futures.append(write_future)
                batch = []
        
        # 残りのバッチを処理
        if batch:
            write_future = self.io_pool.submit(self._write_batch, batch)
            write_futures.append(write_future)
        
        # 全ての書き込みを待機
        for future in as_completed(write_futures):
            future.result()
```

## 5. メモリ管理

### 5.1 メモリ効率的な処理

```python
import gc
from memory_profiler import profile

class MemoryEfficientProcessor:
    """メモリ効率を考慮した並列処理"""
    
    def __init__(self, memory_limit_gb: float = 2.0):
        self.memory_limit_bytes = memory_limit_gb * 1024**3
        self.current_memory_usage = 0
    
    def process_with_memory_limit(self, file_paths: List[str]):
        """メモリ制限付き処理"""
        
        # チャンクサイズの動的調整
        chunk_size = self._calculate_chunk_size(len(file_paths))
        
        for i in range(0, len(file_paths), chunk_size):
            chunk = file_paths[i:i + chunk_size]
            
            # メモリ使用量チェック
            if self._check_memory_pressure():
                # メモリ逼迫時はGCを強制実行
                gc.collect()
                chunk_size = max(1, chunk_size // 2)
            
            # チャンクを処理
            self._process_chunk(chunk)
            
            # 定期的なクリーンアップ
            if i % 100 == 0:
                gc.collect()
    
    def _check_memory_pressure(self) -> bool:
        """メモリ逼迫をチェック"""
        
        memory_info = psutil.virtual_memory()
        used_memory = memory_info.used
        
        return used_memory > self.memory_limit_bytes * 0.8
    
    @profile
    def _process_chunk(self, file_paths: List[str]):
        """メモリプロファイリング付き処理"""
        
        # ストリーミング処理でメモリ使用を最小化
        for file_path in file_paths:
            # ファイルを一時的に処理
            with open(file_path) as f:
                # 大きなファイルは分割して処理
                for chunk in self._read_in_chunks(f):
                    process_chunk(chunk)
```

### 5.2 リソースプール管理

```python
from contextlib import contextmanager
import threading

class ResourcePool:
    """リソースプールの管理"""
    
    def __init__(self, max_resources: int):
        self.semaphore = threading.Semaphore(max_resources)
        self.resources = []
        self.lock = threading.Lock()
    
    @contextmanager
    def acquire_resource(self):
        """リソースの取得と解放"""
        
        self.semaphore.acquire()
        resource = None
        
        try:
            with self.lock:
                if self.resources:
                    resource = self.resources.pop()
                else:
                    resource = self._create_resource()
            
            yield resource
            
        finally:
            with self.lock:
                self.resources.append(resource)
            self.semaphore.release()
    
    def _create_resource(self):
        """新しいリソースを作成"""
        return create_neo4j_session()

# 使用例
neo4j_pool = ResourcePool(max_resources=10)

with neo4j_pool.acquire_resource() as session:
    # セッションを使用
    session.run("CREATE (n:Node)")
```

## 6. エラーハンドリング

### 6.1 並列処理でのエラー処理

```python
class ParallelErrorHandler:
    """並列処理でのエラーハンドリング"""
    
    def __init__(self, retry_count: int = 3):
        self.retry_count = retry_count
        self.failed_items = []
        self.lock = threading.Lock()
    
    def with_retry(self, func, *args, **kwargs):
        """リトライ付き実行"""
        
        last_error = None
        for attempt in range(self.retry_count):
            try:
                return func(*args, **kwargs)
                
            except Exception as e:
                last_error = e
                if attempt < self.retry_count - 1:
                    # 指数バックオフ
                    sleep_time = 2 ** attempt
                    time.sleep(sleep_time)
        
        # 最終的に失敗
        with self.lock:
            self.failed_items.append({
                "args": args,
                "error": last_error,
                "timestamp": datetime.utcnow()
            })
        
        raise last_error
    
    def process_failed_items(self):
        """失敗したアイテムの再処理"""
        
        with self.lock:
            items = self.failed_items.copy()
            self.failed_items.clear()
        
        for item in items:
            try:
                # 再処理を試行
                self.reprocess_item(item)
            except Exception as e:
                logger.error(f"Failed to reprocess: {e}")
```

## 7. モニタリングと最適化

### 7.1 パフォーマンスモニタリング

```python
class ParallelPerformanceMonitor:
    """並列処理のパフォーマンス監視"""
    
    def __init__(self):
        self.metrics = {
            "processed_files": 0,
            "processing_times": [],
            "worker_utilization": {},
            "memory_usage": [],
            "queue_sizes": []
        }
        self.start_time = time.time()
    
    def record_file_processed(self, file_path: str, duration: float):
        """ファイル処理の記録"""
        
        self.metrics["processed_files"] += 1
        self.metrics["processing_times"].append(duration)
    
    def record_worker_status(self, worker_id: int, status: str):
        """ワーカーステータスの記録"""
        
        if worker_id not in self.metrics["worker_utilization"]:
            self.metrics["worker_utilization"][worker_id] = {
                "busy_time": 0,
                "idle_time": 0
            }
        
        if status == "busy":
            self.metrics["worker_utilization"][worker_id]["busy_time"] += 1
        else:
            self.metrics["worker_utilization"][worker_id]["idle_time"] += 1
    
    def get_performance_summary(self) -> dict:
        """パフォーマンスサマリーの取得"""
        
        total_time = time.time() - self.start_time
        avg_processing_time = (
            sum(self.metrics["processing_times"]) / 
            len(self.metrics["processing_times"])
            if self.metrics["processing_times"] else 0
        )
        
        return {
            "total_files": self.metrics["processed_files"],
            "total_time": total_time,
            "throughput": self.metrics["processed_files"] / total_time,
            "avg_processing_time": avg_processing_time,
            "worker_efficiency": self._calculate_worker_efficiency()
        }
    
    def _calculate_worker_efficiency(self) -> float:
        """ワーカー効率の計算"""
        
        total_busy = sum(
            w["busy_time"] 
            for w in self.metrics["worker_utilization"].values()
        )
        total_time = sum(
            w["busy_time"] + w["idle_time"]
            for w in self.metrics["worker_utilization"].values()
        )
        
        return total_busy / total_time if total_time > 0 else 0
```

### 7.2 自動チューニング

```python
class AutoTuner:
    """並列処理の自動チューニング"""
    
    def __init__(self):
        self.performance_history = []
        self.best_config = None
        self.best_throughput = 0
    
    def tune_parameters(self, file_paths: List[str]):
        """パラメータの自動チューニング"""
        
        # テストする設定の組み合わせ
        test_configs = [
            {"workers": 2, "batch_size": 50},
            {"workers": 4, "batch_size": 100},
            {"workers": 8, "batch_size": 200},
            {"workers": os.cpu_count(), "batch_size": 500}
        ]
        
        # 各設定でベンチマーク
        for config in test_configs:
            # サンプルファイルで性能測定
            sample_files = file_paths[:50]
            
            processor = ParallelProcessor(max_workers=config["workers"])
            monitor = ParallelPerformanceMonitor()
            
            start_time = time.time()
            processor.process_files_parallel(sample_files, process_file)
            duration = time.time() - start_time
            
            throughput = len(sample_files) / duration
            
            if throughput > self.best_throughput:
                self.best_throughput = throughput
                self.best_config = config
        
        logger.info(f"Best config found: {self.best_config}")
        return self.best_config
```

## 8. 推奨設定

### 8.1 ファイル数別推奨設定

```python
RECOMMENDED_CONFIGS = {
    "small": {  # < 50 files
        "strategy": ParallelStrategy.THREAD_POOL,
        "workers": 2,
        "batch_size": 10
    },
    "medium": {  # 50-200 files
        "strategy": ParallelStrategy.PROCESS_POOL,
        "workers": 4,
        "batch_size": 50
    },
    "large": {  # 200-500 files
        "strategy": ParallelStrategy.HYBRID,
        "cpu_workers": os.cpu_count(),
        "io_workers": os.cpu_count() * 2,
        "batch_size": 100
    },
    "xlarge": {  # > 500 files
        "strategy": ParallelStrategy.HYBRID,
        "cpu_workers": os.cpu_count(),
        "io_workers": min(os.cpu_count() * 2, 20),
        "batch_size": 200,
        "enable_streaming": True
    }
}

def get_recommended_config(file_count: int) -> dict:
    """ファイル数に基づく推奨設定を取得"""
    
    if file_count < 50:
        return RECOMMENDED_CONFIGS["small"]
    elif file_count < 200:
        return RECOMMENDED_CONFIGS["medium"]
    elif file_count < 500:
        return RECOMMENDED_CONFIGS["large"]
    else:
        return RECOMMENDED_CONFIGS["xlarge"]
```

この設計により、ast2graphライブラリは大規模ファイル処理でも効率的な並列処理を実現します。